{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TILES\n",
    "## Scraping play-by-play data from stats.nba.com\n",
    "\n",
    "We define two methods, one using BeautifulSoup and the other using the stats.nba.com API (via http://www.gregreda.com/2015/02/15/web-scraping-finding-the-api/).\n",
    "\n",
    "We use the API method.\n",
    "\n",
    "The BeautifulSoup method is preserved for reference.\n",
    "\n",
    "We use the EVENTMSGTYPE column because it encodes the action information. See DOCS.md for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - with stats.nba.com API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# dont hide columns when displaying dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sometimes (often) the link does not open on the first try (a HTTP request exception is thrown)\n",
    "\n",
    "a workaround is to open the link on the browser and then re-run\n",
    "\n",
    "SOLVED: need to provide a user-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get playbyplay data\n",
    "# needs an internet connection\n",
    "\n",
    "# playbyplay_url = 'http://stats.nba.com/stats/playbyplay?StartPeriod=1&EndPeriod=10&GameID=0041500314'\n",
    "# alternative link: 'http://stats.nba.com/stats/playbyplayv2?StartPeriod=1&EndPeriod=10&GameID=0041500314' (more complete data, with player info)\n",
    "\n",
    "base_pbp_url = 'http://stats.nba.com/stats/playbyplay?StartPeriod=1&EndPeriod=10&GameID='\n",
    "\n",
    "\n",
    "# example: get gameIDs for all games of 2015/2016 regular season\n",
    "from tilestools import get_gameID\n",
    "\n",
    "games = []\n",
    "season = 2015\n",
    "typ = 'R'\n",
    "total_games = 100\n",
    "\n",
    "for num in range(1,total_games):\n",
    "    gameid = get_gameID(season, typ, num)\n",
    "    games.append(gameid)\n",
    "\n",
    "# complete URL list\n",
    "\n",
    "pbp_url_list = [base_pbp_url + gameid for gameid in games]\n",
    "\n",
    "\n",
    "df_raw_list = [] \n",
    "j=0\n",
    "for playbyplay_url in pbp_url_list:\n",
    "    j+=1\n",
    "    # request the URL and parse the JSON\n",
    "    head = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:43.0) Gecko/20100101 Firefox/43.0'}\n",
    "    response = requests.get(playbyplay_url, headers=head)\n",
    "    response.raise_for_status() # raise exception if invalid response\n",
    "    header = response.json()['resultSets'][0]['headers']\n",
    "    plays = response.json()['resultSets'][0]['rowSet']\n",
    "\n",
    "    # get the playbyplay data in a dataframe\n",
    "    df_raw_list.append(pd.DataFrame(plays, columns=header))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"({}/{}) Acquired data from {}\".format(j, len(pbp_url_list), playbyplay_url))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should find a way of storing all these game logs, to avoid being dependent on internet connection, the logs availability, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data (including handling missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# define features to consider\n",
    "\n",
    "features = [\"REBOUND_HOME\", \"REBOUND_AWAY\", \n",
    "            \"2PT_MADE_HOME\", \"2PT_MADE_AWAY\",\n",
    "            \"2PT_MISS_HOME\", \"2PT_MISS_AWAY\",\n",
    "            \"3PT_MADE_HOME\", \"3PT_MADE_AWAY\",\n",
    "            \"3PT_MISS_HOME\", \"3PT_MISS_AWAY\",\n",
    "            \"TOV_HOME\", \"TOV_AWAY\",\n",
    "            \"TIMEOUT_HOME\", \"TIMEOUT_AWAY\"\n",
    "            ]\n",
    "\n",
    "\n",
    "\n",
    "# set up master_X, master_y to aggregate information of each game\n",
    "# later, these first lines ust be removed\n",
    "master_X = np.ones((1,len(features)-2)) # -2 because of timeouts, which we don't include in X\n",
    "master_y = np.ones(1)\n",
    "\n",
    "# iterate over all games, which are stored in a list of dataframes\n",
    "\n",
    "j=0\n",
    "for df_raw in df_raw_list:\n",
    "    j+=1\n",
    "    \n",
    "    # create a copy\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # propagate score so that every table has the appropriate score\n",
    "\n",
    "    # set first score to 0-0\n",
    "    df.set_value(0,'SCORE',\"0 - 0\")\n",
    "    # forward-propagate\n",
    "    df.SCORE.fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n",
    "    # regularize eventnum (sometimes jumps a few)\n",
    "    # make it match df lin\n",
    "    df['EVENTNUM'] = pd.Series(df.index)\n",
    "\n",
    "\n",
    "    # let's build a dataframe with only 0 and 1: df_events\n",
    "    # each line is an event and is all zeros and only one 1, corresponding to the event \n",
    "    # df_events contains only events which are selected as relevant\n",
    "    # refer to DOCS.md for documentation on how to deal with EVENTMSGTYPE and other features of original dataframe\n",
    "\n",
    "\n",
    "\n",
    "    df_events = build_df_events(df, features)\n",
    "\n",
    "    X, y = build_data(df_events)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # now we have a one-hot encoded dataframe where every row is an event and is all zeros except for one column,\n",
    "    #     which corresponds to the event\n",
    "    # must decide what to to with rows that are not featured events!\n",
    "\n",
    "#    print(\"All zero rows: {}/{}\".format((df_events==0).all(axis=1).sum(), len(df_events)))\n",
    "\n",
    "\n",
    "    master_X = np.concatenate((master_X, X))\n",
    "    master_y = np.concatenate((master_y, y))\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"({}/{}) Size of X: {} % of timeout events: {:.3f}%\".format(j,len(df_raw_list), len(master_X), 100*sum(master_y==1)/len(master_X)))\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# restore natural nomenclature, deleting first line\n",
    "    \n",
    "X = master_X[1:]\n",
    "y = master_y[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_df_events(df, features):\n",
    "    \"\"\"\n",
    "    Builds a one-hot encoded dataframe from the original dataframe df and a list of relevant features\n",
    "    \n",
    "    Input:\n",
    "        df (pandas dataframe)\n",
    "        features (list): list of features to be retained\n",
    "    \n",
    "    Output:\n",
    "        df_events (pandas dataframe): one-hot encoded dataframe\n",
    "        \n",
    "    Comments:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    number_of_features = len(features)\n",
    "    number_of_events = len(df)\n",
    "\n",
    "\n",
    "    df_events = pd.DataFrame(data=np.zeros((number_of_events, number_of_features)), columns = features)\n",
    "\n",
    "    # rebounds\n",
    "    home_rebound_index = df[(df.EVENTMSGTYPE==4) & (df.HOMEDESCRIPTION.notnull())].index\n",
    "    df_events.loc[home_rebound_index,\"REBOUND_HOME\"] = 1.0\n",
    "\n",
    "    away_rebound_index = df[(df.EVENTMSGTYPE==4) & (df.VISITORDESCRIPTION.notnull())].index\n",
    "    df_events.loc[away_rebound_index,\"REBOUND_AWAY\"] = 1.0\n",
    "\n",
    "    # 2PT made\n",
    "    idx = df[(df[\"EVENTMSGTYPE\"]==1) & (df.HOMEDESCRIPTION.notnull()) & (df.HOMEDESCRIPTION.str.contains(\"^((?!3PT).)*$\"))].index\n",
    "    df_events.loc[idx,\"2PT_MADE_HOME\"] = 1.0\n",
    "\n",
    "    idx = df[(df[\"EVENTMSGTYPE\"]==1) & (df.VISITORDESCRIPTION.notnull()) & (df.VISITORDESCRIPTION.str.contains(\"^((?!3PT).)*$\"))].index\n",
    "    df_events.loc[idx,\"2PT_MADE_AWAY\"] = 1.0\n",
    "\n",
    "    # 3PT made\n",
    "    idx = df[(df.EVENTMSGTYPE==1) & (df.HOMEDESCRIPTION.notnull()) & (df.HOMEDESCRIPTION.str.contains(\"3PT\"))].index\n",
    "    df_events.loc[idx,\"3PT_MADE_HOME\"] = 1.0\n",
    "\n",
    "    idx = df[(df.EVENTMSGTYPE==1) & (df.VISITORDESCRIPTION.notnull()) & (df.VISITORDESCRIPTION.str.contains(\"3PT\"))].index\n",
    "    df_events.loc[idx,\"3PT_MADE_AWAY\"] = 1.0\n",
    "\n",
    "    # 2PT miss\n",
    "    idx = df[(df_raw[\"EVENTMSGTYPE\"]==2) & (df.HOMEDESCRIPTION.str.contains(\"MISS\")) & (df.HOMEDESCRIPTION.str.contains(\"^((?!3PT).)*$\"))].index\n",
    "    df_events.loc[idx,\"2PT_MISS_HOME\"] = 1.0\n",
    "\n",
    "    idx = df[(df_raw[\"EVENTMSGTYPE\"]==2) & (df.VISITORDESCRIPTION.str.contains(\"MISS\")) & (df.VISITORDESCRIPTION.str.contains(\"^((?!3PT).)*$\"))].index\n",
    "    df_events.loc[idx,\"2PT_MISS_AWAY\"] = 1.0\n",
    "\n",
    "    # 3PT miss\n",
    "    idx = df[(df_raw[\"EVENTMSGTYPE\"]==2) & (df.HOMEDESCRIPTION.str.contains(\"MISS\")) & (df.HOMEDESCRIPTION.str.contains(\"3PT\"))].index\n",
    "    df_events.loc[idx,\"3PT_MISS_HOME\"] = 1.0\n",
    "\n",
    "    idx = df[(df_raw[\"EVENTMSGTYPE\"]==2) & (df.VISITORDESCRIPTION.str.contains(\"MISS\")) & (df.VISITORDESCRIPTION.str.contains(\"3PT\"))].index \n",
    "    df_events.loc[idx,\"3PT_MISS_AWAY\"] = 1.0\n",
    "\n",
    "    # turnovers\n",
    "    idx = df[(df.EVENTMSGTYPE==5) & (df.HOMEDESCRIPTION.str.contains(\"Turnover\"))].index\n",
    "    df_events.loc[idx,\"TOV_HOME\"] = 1.0\n",
    "\n",
    "    idx = df[(df.EVENTMSGTYPE==5) & (df.VISITORDESCRIPTION.str.contains(\"Turnover\"))].index\n",
    "    df_events.loc[idx,\"TOV_AWAY\"] = 1.0\n",
    "\n",
    "\n",
    "    # timeouts\n",
    "    idx = df[(df.EVENTMSGTYPE==9) & (df.HOMEDESCRIPTION.notnull())].index\n",
    "    df_events.loc[idx,\"TIMEOUT_HOME\"] = 1.0\n",
    "\n",
    "    idx = df[(df.EVENTMSGTYPE==9) & (df.VISITORDESCRIPTION.notnull())].index\n",
    "    df_events.loc[idx,\"TIMEOUT_AWAY\"] = 1.0    \n",
    "\n",
    "\n",
    "    return df_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data(df_events, window_size=10):\n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "    \n",
    "    Output:\n",
    "        X (numpy matrix): the design matrix\n",
    "        y (numpy array): the labels vector\n",
    "    \n",
    "    \"\"\"\n",
    "    # create design matrix X and labels y\n",
    "\n",
    "    # we define a window of size window_size and select every set of window_size points as a training example\n",
    "    # the label is whether the index of the next event is a timeout or not\n",
    "\n",
    "    # this assumes that there wasn't a timeout in the first window_size events\n",
    "\n",
    "    number_of_events, number_of_features = df_events.shape\n",
    "\n",
    "    num_training_examples = number_of_events - window_size\n",
    "\n",
    "    X = np.zeros((num_training_examples, number_of_features))\n",
    "\n",
    "    timeout_events = list(df_events[(df_events.TIMEOUT_HOME==1) | (df_events.TIMEOUT_AWAY==1) ].index)\n",
    "\n",
    "    idx=0\n",
    "    for event_id in range(window_size, number_of_events):\n",
    "        before_events = df_events.iloc[event_id-window_size : event_id]\n",
    "        X[idx,:] = sum(before_events)\n",
    "        idx+=1\n",
    "\n",
    "    # labels\n",
    "    # create list of timeout events indexes in new setting\n",
    "    # e.g. if window_size=10 and the 11th event was a timeout, then y[0] must be 1\n",
    "    timeout_events_idx = [idx-window_size for idx in timeout_events]\n",
    "\n",
    "    # create labels\n",
    "    y = zeros(num_training_examples)\n",
    "    y[timeout_events_idx] = 1.0\n",
    "\n",
    "\n",
    "    # remove last two columns in  X, which are the timeouts\n",
    "    X = np.delete(X, np.s_[-2:], axis=1)\n",
    "\n",
    "    # sanity check\n",
    "    assert len(X) == len(y)    \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Visualizing X\n",
    "## select N random training examples to visualize\n",
    "\n",
    "N = 50\n",
    "\n",
    "idx = np.random.choice(range(len(X)), size=N, replace=False)\n",
    "plt.imshow(X[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Visualize X when it's a timeout event\n",
    "\n",
    "idx = y==1\n",
    "plt.imshow(X[idx])\n",
    "\n",
    "# need more timeouts! this should show some structure!!!!!\n",
    "\n",
    "X[y==1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must set up cross-validation to understand how our model is performing.\n",
    "\n",
    "Because we have very skewed data, i.e. many examples of one class (non-timeout) and few of the other (timeout), we cannot simply randomly split the data into training and test sets because we run the risk of putting very few (or none at all) positive examples in one of the sets.\n",
    "\n",
    "To address this problem we do something called stratified sampling, which simply means selecting data points randomly but preserving the relative class frequencies.\n",
    "\n",
    "So for example, say we have 1000 data points where 10 are positive examples and 990 are negative examples. We want to split the data in 80% training and 20% test data. Then what we do is we randomly select 80% of the 990 negative examples, plus 80% of the 10 positive example, and that is our training set; the remaining 20% of the 990 negative examples plus 20% of the 10 positive examples are our test set.\n",
    "\n",
    "This we we at least ensure we're not making the problem of over-representation of one class even worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's start by standardizing our features\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# standardize features\n",
    "sc = StandardScaler(copy=False)\n",
    "sc.fit_transform(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's try oversampling X, i.e. repeating the timeout data points\n",
    "# we'll do this up to a point where we get a 90-10 imbalance\n",
    "# since we start with about 2.5% positive y, \n",
    "\n",
    "positive_X = X[y==1]\n",
    "positive_y = y[y==1]\n",
    "\n",
    "positive_y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "addons = 25\n",
    "for i in range(0,addons):\n",
    "    X = np.concatenate((X,positive_X))\n",
    "    y = np.concatenate((y,positive_y))\n",
    "    \n",
    "sum(y==1)/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we'll split our data in 80-20 fashion\n",
    "# because we're dealing with skewed classes, must use stratified sampling\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.20, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our first model - Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, let's see training and test scores (mean accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.score(X_train,y_train))\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, looks pretty good!\n",
    "\n",
    "Wait a minute... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_predicted = model.predict(X_train)\n",
    "y_test_predicted = model.predict(X_test)\n",
    "print(y_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so this model is just predicting zeros for everything. Not very useful\n",
    "\n",
    "We can use precision and recall to better evaluate its performance\n",
    "\n",
    "Could we also train it to optimize these costs, rather than the typical logloss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use precision and recall metrics\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_test, y_test_predicted)\n",
    "recall = recall_score(y_test, y_test_predicted)\n",
    "\n",
    "precision = precision_score(y_train, y_train_predicted)\n",
    "recall = recall_score(y_train, y_train_predicted)\n",
    "\n",
    "print (precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use PR-score\n",
    "# from now on, this is what we'll use to evaluate models\n",
    "\n",
    "from tilestools import PRscore\n",
    "PRscore(y_test, y_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, shit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, where to go now:\n",
    "\n",
    "1. ~~We clearly need more data - get data from other games~~ DONE\n",
    "2. ~~We clearly need better data - separate features into for and against~~ DONE\n",
    "3. We probably need a better model - would be nice to train with precision/recall rather than accuracy\n",
    "4. Also, we can try under-sampling and over-sampling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference code\n",
    "\n",
    "Code not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Method 1 - with BeautifulSoup\n",
    "Not used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all the info in the website, via inspecting the html code\n",
    "# not used \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"http://stats.nba.com/game/#!/0041500314/playbyplay/\"\n",
    "r  = requests.get(url)\n",
    "\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data)\n",
    "#print(soup.prettify())\n",
    "\n",
    "table = soup.find('table', attrs={'class': \"table\"})\n",
    "#print(table)\n",
    "\n",
    "\n",
    "rows = table.findAll('tr')\n",
    "for tr in rows:\n",
    "    cols = tr.findAll('td')\n",
    "    for td in cols:\n",
    "        text = td.find(text=True) + ';'\n",
    "#        print (text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# timeouts baby\n",
    "\n",
    "df_home  = df[df.HOMEDESCRIPTION.notnull()]\n",
    "df_home_timeouts=df_home[df_home.HOMEDESCRIPTION.str.contains(\"Timeout\")]\n",
    "\n",
    "df_away  = df[df.VISITORDESCRIPTION.notnull()]\n",
    "df_away_timeouts=df_away[df_away.VISITORDESCRIPTION.str.contains(\"Timeout\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_timeouts = pd.concat([df_home_timeouts, df_away_timeouts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_timeouts.sort_values(by=\"EVENTNUM\")[[\"EVENTMSGTYPE\", \"HOMEDESCRIPTION\", \"VISITORDESCRIPTION\", \"SCORE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explore data\n",
    "# fields to explore: EVENTMSGACTIONTYPE, EVENTMSGTYPE\n",
    "\n",
    "field_to_explore = \"EVENTMSGTYPE\"\n",
    "fields_to_show = [field_to_explore] + [\"EVENTMSGACTIONTYPE\", \"HOMEDESCRIPTION\", \"NEUTRALDESCRIPTION\", \"VISITORDESCRIPTION\"]\n",
    "\n",
    "a = df[fields_to_show]\n",
    "\n",
    "max_ = df[field_to_explore].max()\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "n=1\n",
    "@interact(emat = (0,int(max_)))\n",
    "def show_df(emat=n):\n",
    "    display(a[a[field_to_explore] == emat])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (python3)",
   "language": "python",
   "name": "py3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
