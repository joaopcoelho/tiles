{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TILES\n",
    "## Scraping play-by-play data from stats.nba.com\n",
    "\n",
    "We define two methods, one using BeautifulSoup and the other using the stats.nba.com API (via http://www.gregreda.com/2015/02/15/web-scraping-finding-the-api/).\n",
    "\n",
    "We use the API method.\n",
    "\n",
    "The BeautifulSoup method is preserved for reference.\n",
    "\n",
    "We use the EVENTMSGTYPE column because it encodes the action information. See DOCS.md for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - with stats.nba.com API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "from tilestools import print_scores\n",
    "\n",
    "\n",
    "# dont hide columns when displaying dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add condition: if data available in data/, dont download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get playbyplay data\n",
    "# first try to load data from data/ folder, if not available then download\n",
    "# if download necessary, needs an internet connection\n",
    "\n",
    "# playbyplay_url = 'http://stats.nba.com/stats/playbyplay?StartPeriod=1&EndPeriod=10&GameID=0041500314'\n",
    "# alternative link: 'http://stats.nba.com/stats/playbyplayv2?StartPeriod=1&EndPeriod=10&GameID=0041500314' (more complete data, with player info)\n",
    "\n",
    "from tilestools import get_gameID\n",
    "\n",
    "# games to extract\n",
    "seasons = [2015]\n",
    "types = ['R']\n",
    "total_games = 50\n",
    "\n",
    "basefolder = 'data/'\n",
    "\n",
    "games_downloaded = []\n",
    "games_to_download = []\n",
    "\n",
    "for num in range(1,total_games):\n",
    "    for seas in seasons:\n",
    "        for t in types:\n",
    "            gameid = get_gameID(seas, t, num)\n",
    "            gameid_filename = os.path.join(basefolder, gameid + \".csv\")\n",
    "            \n",
    "            if os.path.isfile(gameid_filename):\n",
    "                games_downloaded.append(gameid_filename)\n",
    "            else:\n",
    "                games_to_download.append(gameid)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import games_downloaded\n",
    "\n",
    "df_raw_list = []\n",
    "\n",
    "j=0\n",
    "for f in games_downloaded:\n",
    "    j+=1\n",
    "    \n",
    "    df = pd.read_csv(f)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    df_raw_list.append(df)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"({}/{}) Read csv data from {}\".format(j, len(games_downloaded), f))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "\n",
    "# download games_to_download\n",
    "\n",
    "# complete URL list\n",
    "base_pbp_url = 'http://stats.nba.com/stats/playbyplay?StartPeriod=1&EndPeriod=10&GameID='\n",
    "pbp_url_list = [base_pbp_url + gameid for gameid in games_to_download]\n",
    "\n",
    "\n",
    "j=0\n",
    "for playbyplay_url in pbp_url_list:\n",
    "    j+=1\n",
    "    # request the URL and parse the JSON\n",
    "    head = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:43.0) Gecko/20100101 Firefox/43.0'}\n",
    "    response = requests.get(playbyplay_url, headers=head)\n",
    "    response.raise_for_status() # raise exception if invalid response\n",
    "    header = response.json()['resultSets'][0]['headers']\n",
    "    plays = response.json()['resultSets'][0]['rowSet']\n",
    "\n",
    "    # get the playbyplay data in a dataframe\n",
    "    df_raw_list.append(pd.DataFrame(plays, columns=header))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"({}/{}) Acquired data from {}\".format(j, len(pbp_url_list), playbyplay_url))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write acquired data to csv\n",
    "# to use, set FLAG_write_to_csv to True\n",
    "# write only games in games_to_download\n",
    "\n",
    "FLAG_write_to_csv = True\n",
    "\n",
    "basefolder = \"data/\"\n",
    "if FLAG_write_to_csv is True:\n",
    "    i=0\n",
    "    for g in games_to_download:\n",
    "        filename = os.path.join(basefolder, g+'.csv')\n",
    "        df_raw_list[i].to_csv(filename)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(\"({}/{}) Write CSV file {}\".format(i+1, len(games_to_download), filename))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "else:\n",
    "    print(\"No CSV files written (FLAG_write_to_csv set to False)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data (including handling missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# define features to consider\n",
    "\n",
    "features = [\"REBOUND_HOME\", \"REBOUND_AWAY\", \n",
    "            \"2PT_MADE_HOME\", \"2PT_MADE_AWAY\",\n",
    "            \"2PT_MISS_HOME\", \"2PT_MISS_AWAY\",\n",
    "            \"3PT_MADE_HOME\", \"3PT_MADE_AWAY\",\n",
    "            \"3PT_MISS_HOME\", \"3PT_MISS_AWAY\",\n",
    "            \"TOV_HOME\", \"TOV_AWAY\",\n",
    "            \"TIMEOUT_HOME\", \"TIMEOUT_AWAY\"\n",
    "            ]\n",
    "\n",
    "\n",
    "\n",
    "# set up master_X, master_y to aggregate information of each game\n",
    "# later, these first lines ust be removed\n",
    "master_X = np.ones((1,len(features)-2)) # -2 because of timeouts, which we don't include in X\n",
    "master_y = np.ones(1)\n",
    "\n",
    "# iterate over all games, which are stored in a list of dataframes\n",
    "from tilestools import build_df_events, build_cumulative_data\n",
    "\n",
    "j=0\n",
    "for df_raw in df_raw_list:\n",
    "    j+=1\n",
    "    \n",
    "    # create a copy\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # propagate score so that every table has the appropriate score\n",
    "\n",
    "    # set first score to 0-0\n",
    "    df.set_value(0,'SCORE',\"0 - 0\")\n",
    "    # forward-propagate\n",
    "    df.SCORE.fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n",
    "    # regularize eventnum (sometimes jumps a few)\n",
    "    # make it match df lin\n",
    "    df['EVENTNUM'] = pd.Series(df.index)\n",
    "\n",
    "\n",
    "    # let's build a dataframe with only 0 and 1: df_events\n",
    "    # each line is an event and is all zeros and only one 1, corresponding to the event \n",
    "    # df_events contains only events which are selected as relevant\n",
    "    # refer to DOCS.md for documentation on how to deal with EVENTMSGTYPE and other features of original dataframe\n",
    "\n",
    "\n",
    "\n",
    "    df_events = build_df_events(df, features)\n",
    "\n",
    "    X, y = build_cumulative_data(df_events, window_size=8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # now we have a one-hot encoded dataframe where every row is an event and is all zeros except for one column,\n",
    "    #     which corresponds to the event\n",
    "    # must decide what to to with rows that are not featured events!\n",
    "\n",
    "#    print(\"All zero rows: {}/{}\".format((df_events==0).all(axis=1).sum(), len(df_events)))\n",
    "\n",
    "\n",
    "    master_X = np.concatenate((master_X, X))\n",
    "    master_y = np.concatenate((master_y, y))\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(\"({}/{}) Size of X: {} % of timeout events: {:.3f}%\".format(j,len(df_raw_list), len(master_X), 100*sum(master_y==1)/len(master_X)))\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# restore natural nomenclature, deleting first line\n",
    "    \n",
    "X = master_X[1:]\n",
    "y = master_y[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Visualizing X\n",
    "## select N random training examples to visualize\n",
    "\n",
    "N = 50\n",
    "\n",
    "idx = np.random.choice(range(len(X)), size=N, replace=False)\n",
    "plt.imshow(X[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Visualize X when it's a timeout event\n",
    "\n",
    "idx = y==1\n",
    "plt.imshow(X[idx][:N])\n",
    "\n",
    "sum(idx)\n",
    "\n",
    "# need more timeouts! this should show some structure!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must set up cross-validation to understand how our model is performing.\n",
    "\n",
    "Because we have very skewed data, i.e. many examples of one class (non-timeout) and few of the other (timeout), we cannot simply randomly split the data into training and test sets because we run the risk of putting very few (or none at all) positive examples in one of the sets.\n",
    "\n",
    "To address this problem we do something called stratified sampling, which simply means selecting data points randomly but preserving the relative class frequencies.\n",
    "\n",
    "So for example, say we have 1000 data points where 10 are positive examples and 990 are negative examples. We want to split the data in 80% training and 20% test data. Then what we do is we randomly select 80% of the 990 negative examples, plus 80% of the 10 positive example, and that is our training set; the remaining 20% of the 990 negative examples plus 20% of the 10 positive examples are our test set.\n",
    "\n",
    "This we we at least ensure we're not making the problem of over-representation of one class even worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's start by standardizing our features\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# standardize features\n",
    "sc = StandardScaler(copy=False)\n",
    "sc.fit_transform(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we'll split our data in 80-20 fashion\n",
    "# because we're dealing with skewed classes, must use stratified sampling\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.20, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first model: Logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train);\n",
    "\n",
    "\n",
    "print_scores(model, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with oversampling\n",
    "\n",
    "Because we have too few positive samples, need to rebalance data set\n",
    "\n",
    "We try oversampling, i.e. repeating the positive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logistic regression with oversampling\n",
    "\n",
    "scores = []\n",
    "\n",
    "for K in range(50):\n",
    "    # K is the number of times we add the positive examples to the labels vector\n",
    "\n",
    "    # we'll split our data in 80-20 fashion\n",
    "    # because we're dealing with skewed classes, must use stratified sampling\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "\n",
    "\n",
    "    # let's try oversampling X, i.e. repeating the timeout data points\n",
    "    # we'll do this up to a point where we get a 90-10 imbalance\n",
    "    # since we start with about 2.5% positive y, \n",
    "\n",
    "    positive_X = X_train[y_train==1]\n",
    "    positive_y = y_train[y_train==1]\n",
    "\n",
    "    new_X_train = X_train.copy()\n",
    "    new_y_train = y_train.copy()\n",
    "\n",
    "    addons = K\n",
    "    for i in range(0,addons):\n",
    "        new_X_train = np.concatenate((new_X_train,positive_X))\n",
    "        new_y_train = np.concatenate((new_y_train,positive_y))\n",
    "\n",
    "    X_train=new_X_train\n",
    "    y_train=new_y_train\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # fit model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train,y_train);\n",
    "\n",
    "    y_test_predicted = model.predict(X_test)\n",
    "\n",
    "    from tilestools import F1score\n",
    "    f1 = F1score(y_test, y_test_predicted)\n",
    "    \n",
    "    pct_positive_examples = len(y_train[y_train==1])/len(y_train)\n",
    "    \n",
    "    scores.append((pct_positive_examples,f1))\n",
    "    \n",
    "plt.plot(*zip(*scores))\n",
    "plt.xlabel(\"% of positive examples in training set\")\n",
    "plt.ylabel(\"F1 score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, where to go now:\n",
    "\n",
    "1. ~~We clearly need more data - get data from other games~~ DONE\n",
    "2. ~~We clearly need better data - separate features into for and against~~ DONE\n",
    "3. We probably need a better model - would be nice to train with precision/recall rather than accuracy\n",
    "4. Also, we can try under-sampling and over-sampling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference code\n",
    "\n",
    "Code not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Method 1 - with BeautifulSoup\n",
    "Not used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all the info in the website, via inspecting the html code\n",
    "# not used \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"http://stats.nba.com/game/#!/0041500314/playbyplay/\"\n",
    "r  = requests.get(url)\n",
    "\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data)\n",
    "#print(soup.prettify())\n",
    "\n",
    "table = soup.find('table', attrs={'class': \"table\"})\n",
    "#print(table)\n",
    "\n",
    "\n",
    "rows = table.findAll('tr')\n",
    "for tr in rows:\n",
    "    cols = tr.findAll('td')\n",
    "    for td in cols:\n",
    "        text = td.find(text=True) + ';'\n",
    "#        print (text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# timeouts baby\n",
    "\n",
    "df_home  = df[df.HOMEDESCRIPTION.notnull()]\n",
    "df_home_timeouts=df_home[df_home.HOMEDESCRIPTION.str.contains(\"Timeout\")]\n",
    "\n",
    "df_away  = df[df.VISITORDESCRIPTION.notnull()]\n",
    "df_away_timeouts=df_away[df_away.VISITORDESCRIPTION.str.contains(\"Timeout\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_timeouts = pd.concat([df_home_timeouts, df_away_timeouts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_timeouts.sort_values(by=\"EVENTNUM\")[[\"EVENTMSGTYPE\", \"HOMEDESCRIPTION\", \"VISITORDESCRIPTION\", \"SCORE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explore data\n",
    "# fields to explore: EVENTMSGACTIONTYPE, EVENTMSGTYPE\n",
    "\n",
    "field_to_explore = \"EVENTMSGTYPE\"\n",
    "fields_to_show = [field_to_explore] + [\"EVENTMSGACTIONTYPE\", \"HOMEDESCRIPTION\", \"NEUTRALDESCRIPTION\", \"VISITORDESCRIPTION\"]\n",
    "\n",
    "a = df[fields_to_show]\n",
    "\n",
    "max_ = df[field_to_explore].max()\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "n=1\n",
    "@interact(emat = (0,int(max_)))\n",
    "def show_df(emat=n):\n",
    "    display(a[a[field_to_explore] == emat])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (python3)",
   "language": "python",
   "name": "py3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
